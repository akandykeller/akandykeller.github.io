---
layout: post
title: Traveling Waves Encode the Recent Past and Enhance Sequence Learning
<!-- image: /assets/img/research/wrnn/waves.png -->
sitemap: false
comments: true
---
<!-- ![Full-width image](/assets/img/overview_long.png){:.lead width="800" height="100" loading="lazy"} -->
![WaveField](/assets/img/research/wrnn/opening_pic.png){:.lead width="800" height="400" loading="lazy"}
Illustration of three input signals (top) and a corresponding wave-field with induced traveling waves (bottom). From an instantaneous snapshot of the wave-field at each timestep we are able decode both the time of onset and input channel of each input spike. Furthermore, subsequent spikes in the same channel do not overwrite one-another.
{:.figcaption}

Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically grounded hypothesis suggests that the cortical sheet may act like a wave-field capable of storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how both connectivity constraints and initialization play a crucial role in the emergence of wave-like dynamics. We then empirically show how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and perform significantly better than wave-free counterparts. Finally, we explore the implications of this memory storage system on more complex sequence modeling tasks such as sequential image classification and find that wave-based models not only again outperform comparable wave-free RNNs while using significantly fewer parameters, but additionally perform comparably to more complex gated architectures such as LSTMs and GRUs. We conclude with a discussion of the implications of these results for both neuroscience and machine learning.
{:.note title="Abstract"}
**T. Anderson Keller***, [Lyle Muller](https://mullerlab.ca/), [Terrence Sejnowski](https://www.salk.edu/scientist/terrence-sejnowski/), and [Max Welling](https://staff.fnwi.uva.nl/m.welling/)
{:.note title="Authors"}
*Accepted at [ICLR 2024](https://iclr.cc/Conferences/2024)*: <https://openreview.net/forum?id=p4S5Z6Sah4> 
<!-- *Accepted at [](https://www.cosyne.org/) (Poster)* \\
*Conference Abstract:* <https://akandykeller.github.io/papers/LocoRNN.pdf>  -->
{:.note title="Full Paper"}
[https://iclr.cc/media/PosterPDFs/ICLR%202024/17796.png?t=1715213727.2184942](https://iclr.cc/media/PosterPDFs/ICLR%202024/17796.png?t=1715213727.2184942)
{:.note title="Poster"}
[https://github.com/akandykeller/Wave_RNNs](https://github.com/Anon-NeurIPS-2023/Wave-RNN)
{:.note title="Code"}
[https://x.com/t_andy_keller/status/1788917073491472882](https://x.com/t_andy_keller/status/1788917073491472882)
{:.note title="Tweet-print"}

<!-- {:.lead} -->

- Table of Contents
{:toc}
---
layout: post
title: Flow Factorized Representation Learning
<!-- image: /assets/img/research/ffrl/ffrl.png -->
sitemap: false
comments: true
---
<!-- ![Full-width image](/assets/img/overview_long.png){:.lead width="800" height="100" loading="lazy"} -->
![ffrl](/assets/img/research/ffrl/ffrl.png){:.lead width="800" height="400" loading="lazy"}
Illustration of our flow factorized representation learning: at each point in the latent space we have a distinct set of tangent directions $$\nabla u^k$$ which define different transformations we would like to model in the image space. For each path, the latent sample evolves to the target on the potential landscape following dynamic optimal transport.
{:.figcaption}

A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both _disentanglement_ and _equivariance_. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.
{:.note title="Abstract"}
[Yue Song](https://kingjamessong.github.io), **T. Anderson Keller**, [Nicu Sebe](https://www.google.com/search?client=safari&rls=en&q=Nicu+Sebe&ie=UTF-8&oe=UTF-8), and [Max Welling](https://staff.fnwi.uva.nl/m.welling/)
{:.note title="Authors"}
*Accepted at [NeurIPS 2023](https://neurips.cc/Conferences/2023)*: <https://arxiv.org/abs/2309.13167> 
{:.note title="Full Paper"}

 
<!-- {:.lead} -->

<!-- - Table of Contents
{:toc} -->
---
layout: post
title: Fast Weight Long Short-Term Memory
<!-- image: /assets/img/fwlstm/fwlstm.png -->
sitemap: false
comments: true
---
<!-- ![Full-width image](/assets/img/overview_long.png){:.lead width="800" height="100" loading="lazy"} -->
![Self Normalizing Flow Overview](/assets/img/research/fwlstm/fwlstm.png){:.lead width="2974" height="958" loading="lazy"}
A depiction of our gated Long Short-Term Memory cell augmented with an associative memory matrix A which is updated via the outer-product of the gated LSTM cell's output.
{:.figcaption}

Associative memory using fast weights is a short-term memory mechanism that substantially improves the memory capacity and time scale of recurrent neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs. In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories. We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties. 
{:.note title="Abstract"}
**T. Anderson Keller**, [Sharath Nittur Sridhar](https://scholar.google.com/citations?user=OZCldmQAAAAJ&hl=en), [Xin Wang](https://scholar.google.com/citations?user=8mICcqAAAAAJ&hl=en)
{:.note title="Authors"}
*ArXiv Paper*: <https://arxiv.org/abs/1804.065118> 
{:.note title="Full Paper"}
[https://github.com/akandykeller/fast_weights](https://github.com/akandykeller/fast_weights)
{:.note title="Code"}

<!-- {:.lead} -->

- Table of Contents
{:toc}
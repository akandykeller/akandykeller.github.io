---
layout: post
title: Topographic VAEs learn Equivariant Capsules
<!-- image: /assets/img/research/tvae/comm_diag.png -->
sitemap: false
comments: true
---
<!-- ![Full-width image](/assets/img/overview_long.png){:.lead width="800" height="100" loading="lazy"} -->
![Topographic VAE Overview](/assets/img/research/tvae/comm_diag.png){:.lead width="3460" height="1331" loading="lazy"}
Overview of the Topographic VAE with shifting temporal coherence. The combined color/rotation transformation in input space $$\tau_g$$ becomes encoded as a $$\mathrm{Roll}$$ within the equivariant capsule dimension. The model is thus able decode unseen sequence elements by encoding a partial  sequence and rolling activations within the capsules. We see this completes a commutative diagram.
{:.figcaption}



In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. "capsules") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.
{:.note title="Abstract"}
**T. Anderson Keller**, [Max Welling](https://staff.fnwi.uva.nl/m.welling/)
{:.note title="Authors"}
*ArXiv Paper*: <https://arxiv.org/abs/2109.01394>  \\
*Accepted at:* [NeurIPS 2021](https://nips.cc/Conferences/2021/) \\
<!-- *Conference Presentation*: [SlidesLive (10-minutes)](https://recorder-v3.slideslive.com/?share=52300&s=abc00686-3e8e-49d5-b4de-d73c0d53e30e) -->
{:.note title="Full Paper"}
Yannic Kilcher: [https://www.youtube.com/watch?v=pBau7umFhjQ](https://www.youtube.com/watch?v=pBau7umFhjQ)
{:.note title="Media Coverage"}
[Github.com/AKAndykeller/TopographicVAE](https://github.com/akandykeller/TopographicVAE)
{:.note title="Code"}

## Video Summary
<iframe width="894" height="503" src="https://www.youtube.com/embed/8QJmO6u0SwM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Tweet-print
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Together with <a href="https://twitter.com/wellingmax?ref_src=twsrc%5Etfw">@wellingmax</a>, we think deep learning needs more organization and structure... topographic organization and equivariant structure üòÅ<br><br>Introducing our new paper:<br>Topographic VAEs learn Equivariant Capsules<br>üìÉ<a href="https://t.co/n1V04jFBAS">https://t.co/n1V04jFBAS</a><br>üß¨<a href="https://t.co/iV45kMkbsV">https://t.co/iV45kMkbsV</a><br><br>1/6 <a href="https://t.co/tRraSncNqs">pic.twitter.com/tRraSncNqs</a></p>&mdash; Andy Keller (@t_andy_keller) <a href="https://twitter.com/t_andy_keller/status/1435189824424710145?ref_src=twsrc%5Etfw">September 7, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<!-- {:.lead} -->
  
<!-- - Table of Contents
{:toc}

## Equivariance

### What is an Equivariant Capsule?


## Topographic Organization

### Orientation 'Hypercolumns' in the Brain

### Equiviariance $$\Longleftrightarrow$$ Topographic Organization?



## How to _Induce_ Topographic Organization

### Independant Subspace Analisys learns 

## How to learn 
 -->

<!-- ## What is Topographic Organization?
At an abstract level, topographic organization in a neural network can be defined as the arragnement of neurons in an (n-dimensional) grid such that nearby neurons in this topology tend to have greater similarity than spatially distant neurons according to some desired similarity metric. This property is observed in nature across a range of species and for a diversity of stimuli, leading researchers to wonder about the underlying influences and mechanisms responsible for such organization, as well as if there are computational benefits. 
 
### Topographic Organization in the Brain
![orientation_columns](/assets/img/research/tvae/orientation_columns.png){: width="200" style="float: right;" loading="lazy"}
In the brain, topographic organization is more elaborate and widespread than the simple correlation of local activations. The most prominent example of this is the retinotopic maps of the primary visual cortex, whereby the visual field is mapped onto the cortical sufrace (albeit somewhat warped). This organization is reminiscent of the structured representations yeilded by convolutional maps, and indeed is claimed to be the motivation for early convolutional neural networks.

One specific type of topographic organization can be seen with orientation columns. 

*Orientation columns* and their arrangement into *hypercolumns* can be seen as one of earliest discovered and simplest examples of topographic organization.  The figure on the right ([source](http://www.iiserpune.ac.in/~raghav/pdfs/neurobiology2/Lecture5_Vision4_LGN_VisualCortex.pdf)) depecits such organization, showing a section of the primary visual cortex, with colors representing selectivity of neurons to different orientations of lines. We observe that orientation selectivity changes smoothly over the cortical surface, and that it is possible to subdivide the cortical surface into groups ('hypercolumns') such that each group contains roughly a full set rotation angles. Hubel & Weisel called such units 

"a  small  machine that looks after all values of a given variable" -- Hubel

We will get back to orientation hypercolumns in the next section, but we first note that topographic organization certainly does not stop here. 

Infact, from the simple orientation of lines[^1] to the complex semantics of natural language[^2], organization of cortical activity is observed for a diversity of stimuli and across a range of species. Given such strong and ubiquitous observations, it seems only natural to wonder about the computational benefits of such organization, and if the machine learning community can take advantage of such design principles to develop better inductive priors for deep neural network architectures.
 
### Topographic Generative Models
Given such strong and ubiquitous observations from biology, it seems only natural to wonder about the computational benefits of such organization, and if the machine learning community can take advantage of such design principles to develop better inductive priors for deep neural network architectures. In prior work, many such as ... have taken inspiration from such topographic organization to introduce topographic generative models.
 
 
### Physical & Computational Benefits
Wiring length min, Increased independance / removal of higher order correlations with D.N., Learning or invariances.
 
 
 
### How is Equivariance Related?
One inductive prior which has gained popularity in recent years is that of equivariance. At a high level, a representation is said to be equivariant if it transforms in a known predictable manner for a given transformation of the input. A fundamental method for constructing equivariant representations is through structured parameter sharing, constrained by the underlying desired transformation group \cite{cohen2016group, ravanbakhsh2017equivariance, finzi2020generalizing, finzi2021emlp}. The most well known example of an equivariant map is the convolution operation, which is equivariant to translation. One can think of a convolutional layer as a function which shares the same feature extractor parameters over all elements of the translation group, i.e. all spatial locations. Similarly, a model which is equivariant to rotation is one which shares parameters across all rotations. Existing group equivariant neural networks \cite{cohen2016group} therefore propose to maintain 'capsules' of tied-parameters which are correlated by the action of the group. In other words, a rotation equivariant convolutional neural network would maintain (weight-sharing) rotated copies of each of its filters, grouped together implicitly by the act of rotation.
 
These sets of transformed weights, which we refer to as 'equivariant capsules', are reminiscent of a type of topographic organization observed in the primary visual cortex (V1), namely orientation columns[^1], where, at a high level, neurons are observed to group into rings of smoothly rotating orientation selectivity. The insight of this connection between topographic organization and equivariance leads directly to the model presented herein. Specifically, in the following, we will introduce the Topographic VAE, a model which achieves such a union between these two concepts, leveraging topographic organization both over space and time to learn approximately equivariant capsules in an entirely unsupervised manner from observed transformation sequences.  -->
 
<!-- One prominent explanation for topographic organization is that it is the solution to a 'wiring cost' minimization problem[^3]. As Rebescca Schwarzlose explains in her book "Brainscapes"[^4], if every neuron in the human brain were randomly connected to all other neurons, it would need to be 20 Km in width to fit all the connections. It is thus clear that local connectivity is of paramount importance, and so neurons which must communicate frequently (likely those which represent correlated information) should be spatially close.  -->
 
<!-- However, there also exist a number of computational explanations for why topographic organization may emerge, and interestingtly, these models may simultaneously minimize wiring length. Fundamentally, most such computational models rely on the  -->
 
<!-- One of these is the principle of 'redundancy reduction'[^5], which resulted in a generative model called Topographic Independant Component Analysis (TICA)[^6]. Succintly,    -->
 
 
<!-- ## Behind the Curtain
 
The ideas presented in this work are founded on an extensive range or prior work including Independant Component Analysis, Slow Feature Analysis, and Group Equivariant Neural Networks. Due to the 'less-than-academic' nature of this blog post, we will point the reader to our full paper for an extensive overview of the related work and our contributions.   -->

<!-- 
## How to Topograph-y your VAE
Before we can start to talk about equivariance or rotating lines, we need an efficient method for learning topographic representations, ideally within deep convolutional neural networks. Unfortunately, existing methods such as Topographic ICA only apply to the linear setting and additionally are typically trained with heuristic approximation to the likelihood, or unstable energy based modeling techniques. Rather than try to scale such techniques up, we took a step back to look at the generative model behing Topographic ICA and realized that it could be decomposed as a hierarchical generative model composed entirely of independant Gaussian random variables -- and thereby ammenable to variational inference.
 
### Topograpy as Hierarchy
One way in which topographic generative models can be defined is 
 
### Constructing the Topographic Product of Student's-t
asd
 
### The Topographic VAE
asd
 
## Learning Transformations
asd
 
 
 
## Experiments
 
## Discussion
 
 
[^0]: HUBEL, D. H., & WIESEL, T. N. (1962). Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of physiology, 160(1), 106‚Äì154. https://doi.org/10.1113/jphysiol.1962.sp006837
[^1]: Hubel, D. H., & Wiesel, T. N. (1974). Sequence regularity and geometry of orientation columns in the monkey striate cortex. The Journal of comparative neurology, 158(3), 267‚Äì293. https://doi.org/10.1002/cne.901580304
[^2]: Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), 453‚Äì458. https://doi.org/10.1038/nature17637
[^3]: Koulakov, A. A., & Chklovskii, D. B. (2001). Orientation preference patterns in mammalian visual cortex: a wire length minimization approach. Neuron, 29(2), 519‚Äì527. https://doi.org/10.1016/s0896-6273(01)00223-9
[^4]: Schwarzlose, R. (2021). Brainscapes: The warped, wondrous maps written in your brain-and how they guide you. Houghton Mifflin Harcourt.
[^5]: Barlow, H. (2012-09-28). Possible Principles Underlying the Transformations of Sensory Messages. In Sensory Communication. : The MIT Press. Retrieved 16 Jul. 2021, from https://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262518420.001.0001/upso-9780262518420-chapter-13. -->


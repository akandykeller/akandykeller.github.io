---
layout: post
title: Topographic Coupled Oscillator Networks Learn Transformations as Traveling Waves
<!-- image: /assets/img/research/waves/Waves.gif -->
sitemap: false
comments: true
---
<!-- ![Full-width image](/assets/img/overview_long.png){:.lead width="800" height="100" loading="lazy"} -->
![MNIST_Waves_Recon](/assets/img/research/waves/Waves.gif){:.lead width="3460" height="1331" loading="lazy"}
Observed transformation (left), Topographic Latent Variables (middle), and Reconstruction (right). We see the Topographic Coupled Oscillator Network learns to encode the observed transformations as traveling waves. In our paper, we show that such coordinated synchronous dynamics ultimately result in improved robustness and efficiency when similarly modeling smooth continutous transformations as input. 
{:.figcaption}


Structured representations, such as those of convolutional and other equivariant neural networks, have shown increased generalization performance and data efficiency when the integrated structure accurately represents the symmetry transformations present in the data. However, in order to impose such structure, most methods require explicit knowledge of the underlying transformation group which is infeasible for many real-world transformations. In this work, we suggest an extremely general inductive bias  -- that of traveling waves across features space -- may be capable of inducing approximately equivariant structure for arbitrary observed transformations in an unsupervised manner. To demonstrate this, we leverage a biologically relevant dynamical system known to exhibit traveling waves, specifically a network of topographically coupled oscillators, and show that when integrated into a modern deep neural network architecture, such a system does indeed learn to represent observed symmetry transformations as traveling waves in the latent space. The approximate equivariance of the model is verified by artificially inducing waves in the latent space and subsequently decoding to visualize transforming test sequences, implying commutativity of the feature extractor and the transformations in the input and feature spaces. We further demonstrate that our model yields performance competitive with state of the art on a suite of sequence classification and forecasting benchmarks while simultaneously converging more consistently and requiring significantly fewer parameters than its globally coupled counterpart.
{:.note title="Abstract"}
**T. Anderson Keller*** and [Max Welling](https://staff.fnwi.uva.nl/m.welling/)
{:.note title="Authors"}
*Preprint (under review)*: <https://akandykeller.github.io/papers/Waves.pdf> 
{:.note title="Full Paper"}
[Anonymized Github](https://github.com/q2w4/LocoRNN)
{:.note title="Code"}
 

<!-- {:.lead} -->

- Table of Contents
{:toc}